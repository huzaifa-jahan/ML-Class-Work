{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 4404",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCYYrLCZX3W6",
        "outputId": "a0cd94b8-aa73-465f-af3f-a84bd05ccc32"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmx4FaKsY1Gj"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import collections\n",
        "\n",
        "df = pd.read_csv(\"/enwiki8[1].txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVh2ZJzbn-Rd"
      },
      "source": [
        "Q1 freq matrix using sklearn (collab was giving issues storing matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuXx2LDWk978"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "file = open('/content/enwiki8[1].txt')\n",
        "corpus= [file.read()]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())\n",
        "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "X2 = vectorizer2.fit_transform(corpus)\n",
        "print(vectorizer2.get_feature_names())\n",
        "print(X2.toarray())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwbAKCDvgnOt"
      },
      "source": [
        "Q1 Freq Matrix filtering from scratch (did not fully store matrix yet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kye_JsKXZLxF"
      },
      "source": [
        "# Read input file\n",
        "file = open('/enwiki8[1].txt', encoding=\"utf8\")\n",
        "a= file.read()\n",
        "file = open('/wordsim353_human_scores.txt', encoding=\"utf8\")\n",
        "b= file.read()\n",
        "\n",
        "# Stopwords using a file that has unique words per line such as (at, the, there)\n",
        "stopwords = set(line.strip() for line in open('/stopwords.txt'))\n",
        "stopwords = stopwords.union(set(['mr','mrs','one','two','said']))\n",
        "\n",
        "# dictionary\n",
        "wordcount = {}\n",
        "\n",
        "# To eliminate duplicates, remember to split by punctuation, and use case demiliters.\n",
        "for word in a.lower().split() or b.lower().split():\n",
        "    word = word.replace(\".\",\"\")\n",
        "    word = word.replace(\",\",\"\")\n",
        "    word = word.replace(\":\",\"\")\n",
        "    word = word.replace(\"\\\"\",\"\")\n",
        "    word = word.replace(\"!\",\"\")\n",
        "    word = word.replace(\"â€œ\",\"\")\n",
        "    word = word.replace(\"â€˜\",\"\")\n",
        "    word = word.replace(\"*\",\"\")\n",
        "    if word not in stopwords:\n",
        "        if word not in wordcount:\n",
        "            wordcount[word] = 1\n",
        "        else:\n",
        "            wordcount[word] += 1\n",
        "\n",
        "\n",
        "\n",
        "# Print most common word\n",
        "n_print = int(input(\"How many most common words to print: \"))\n",
        "print(\"\\nOK. The {} most common words are as follows\\n\".format(n_print))\n",
        "word_counter = collections.Counter(wordcount)\n",
        "for word, count in word_counter.most_common(n_print):\n",
        "    print(word, \": \", count)\n",
        "\n",
        "# Close the file\n",
        "file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lQUKHLNgiLq"
      },
      "source": [
        "Q2 Truncated SVD (LSA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbwqqo6oeuDh"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import random as sparse_random\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "file = open('/content/enwiki8[1].txt')\n",
        "corpus= [file.read()]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "svd = TruncatedSVD(n_components=1, n_iter=20)\n",
        "svd.fit(X)\n",
        "print(svd.explained_variance_ratio_)\n",
        "print(svd.explained_variance_ratio_.sum())\n",
        "print(svd.singular_values_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzbFpYOi0lO3"
      },
      "source": [
        "Q3 SGD  - calculate the gradient using just a random small part of the observations instead of all "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM4o2dYM0pHw"
      },
      "source": [
        "def gradient_decent(w0, b0, train_data, x_test, y_test, learning_rate):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYQuxKWCp1K1"
      },
      "source": [
        "Q5 T-sne for 2d word vector to attain top 300\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X-U9zjpp387"
      },
      "source": [
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None \n",
        "import numpy as np\n",
        "from gensim.models import word2vec\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file = open('/content/enwiki8[1].txt')\n",
        "corpus= [file.read()]\n",
        "model = word2vec.Word2Vec(corpus, size=10000, window=20, min_count=500, workers=4)\n",
        "tsne_plot(model)\n",
        "\n",
        "def tsne_plot(model):\n",
        "    labels = []\n",
        "    tokens = []\n",
        "\n",
        "    for word in model.wv.vocab:\n",
        "        tokens.append(model[word])\n",
        "        labels.append(word)\n",
        "    \n",
        "    tsne_model = TSNE(perplexity=300, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
        "    new_values = tsne_model.fit_transform(tokens)\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for value in new_values:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "        \n",
        "    plt.figure(figsize=(16, 16)) \n",
        "    for i in range(len(x)):\n",
        "        plt.scatter(x[i],y[i])\n",
        "        plt.annotate(labels[i],xy=(x[i], y[i]),xytext=(5, 2),textcoords='offset points',ha='right',va='bottom')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}